# VLG_SAM
<h1>Training-Free Few-Shot Segmentation via Vision-Language Guided Prompting</a></h1>

PyTorch official implementation of (Euihyun Yoon, Taejin Park and Jaekoo Lee. "Training-Free Few-Shot Segmentation via Vision-Language Guided Prompting" WACV, 2026).

## Description
<img width="1385" height="514" alt="image" src="https://github.com/user-attachments/assets/ea798d39-7094-4b92-83ba-f7100644a8de" />

An overview of our proposed model.Overview of the proposed approach. The approach consists of three stages —TCRP, VLAS, and SMR—which operate entirely training-free by leveraging publicly available foundation models.

<img width="1409" height="783" alt="image" src="https://github.com/user-attachments/assets/7e9c41c5-aac3-45e1-8929-d3b54eb51adc" />
Comparison of Few-shot segmentation performance on various datasets. 


