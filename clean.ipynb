{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f925e476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import logging\n",
    "from PIL import Image\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.ndimage import label as cc_label\n",
    "\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    Owlv2TextModel,\n",
    "    Owlv2Processor,\n",
    "    Owlv2ForObjectDetection,\n",
    "    Owlv2VisionModel,\n",
    "    CLIPProcessor,\n",
    "    CLIPModel\n",
    ")\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from torchvision.transforms.functional import pil_to_tensor, resize, crop\n",
    "from torchvision.ops import box_iou, nms\n",
    "\n",
    "from GEM.dataset_coco import DatasetCOCO\n",
    "import utils_seed\n",
    "from evaluation import Evaluator\n",
    "from logger import Logger, AverageMeter\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def _get_tokens(model, pixel_values, layer_idx):\n",
    "    \"\"\"Extract hidden states from specified layer of vision model.\"\"\"\n",
    "    out = model.vision_model(pixel_values=pixel_values,\n",
    "                             output_hidden_states=True, return_dict=True)\n",
    "    return out.hidden_states[layer_idx]\n",
    "\n",
    "\n",
    "def _resolve_anomaly(tokens, G, top_k=10):\n",
    "    \"\"\"Detect and resolve anomaly tokens using Local Outlier Factor. Replaces outlier tokens with 3x3 neighborhood average.\"\"\"\n",
    "    pt = tokens[:, 1:, :]\n",
    "    B, N, D = pt.shape\n",
    "    new_pt = []\n",
    "    for b in range(B):\n",
    "        feat = pt[b]\n",
    "        lof = LocalOutlierFactor(n_neighbors=30,\n",
    "                                 contamination=float(top_k)/N)\n",
    "        lbl = lof.fit_predict(feat.cpu().numpy())\n",
    "        out_idx = np.where(lbl == -1)[0]\n",
    "        fm = feat.view(G, G, D)\n",
    "        for idx in out_idx:\n",
    "            r,c = divmod(idx, G)\n",
    "            neigh = []\n",
    "            for dr in (-1,0,1):\n",
    "                for dc in (-1,0,1):\n",
    "                    if dr==0 and dc==0: continue\n",
    "                    rr,cc = r+dr, c+dc\n",
    "                    if 0<=rr<G and 0<=cc<G:\n",
    "                        neigh.append(fm[rr,cc])\n",
    "            if neigh:\n",
    "                fm[r,c] = torch.stack(neigh).mean(0)\n",
    "        new_pt.append(fm.reshape(-1,D))\n",
    "    tokens[:,1:,:] = torch.stack(new_pt)\n",
    "    return tokens\n",
    "\n",
    "def _self_adjust(deep_tok, mid_tok, beta=0.4):\n",
    "    \"\"\"Re-aggregate deep tokens using intermediate layer similarity.\"\"\"\n",
    "    deep_norm = F.normalize(deep_tok, dim=-1)\n",
    "    mid_norm  = F.normalize(mid_tok,  dim=-1)\n",
    "    sim = mid_norm @ mid_norm.transpose(1,2)\n",
    "    sim = sim.masked_fill(sim < beta, 0.)\n",
    "    sim = sim / (sim.sum(-1, keepdim=True)+1e-6)\n",
    "    return sim @ deep_tok\n",
    "\n",
    "@torch.inference_mode()\n",
    "def extract_scclip_box_features(model, image_pil:Image.Image,\n",
    "                                boxes:torch.Tensor,\n",
    "                                *, device=\"cpu\",\n",
    "                                mid_idx=8, beta=0.4, top_k=10):\n",
    "    \"\"\"Extract SC-CLIP box features with anomaly resolution and self-adjustment.\"\"\"\n",
    "    inp = resize(image_pil, (224, 224), interpolation=Image.BICUBIC)\n",
    "    px  = pil_to_tensor(inp).float() / 255.\n",
    "    px  = px.unsqueeze(0).to(device)\n",
    "\n",
    "    penult = _get_tokens(model, px, layer_idx=-2)\n",
    "    last   = _get_tokens(model, px, layer_idx=-1)\n",
    "    mid    = _get_tokens(model, px, layer_idx=mid_idx)\n",
    "\n",
    "    G = int(math.sqrt(penult.size(1)-1))\n",
    "    penult = _resolve_anomaly(penult.clone(), G, top_k)\n",
    "\n",
    "    deep_tok = last[:,1:,:]\n",
    "    mid_tok  = mid[:,1:,:]\n",
    "    patch_tok = _self_adjust(deep_tok, mid_tok, beta).squeeze(0)\n",
    "\n",
    "    mid_norm = F.normalize(mid_tok.squeeze(0), dim=-1)\n",
    "    mid_simi = mid_norm @ mid_norm.t()\n",
    "    mid_simi[mid_simi < beta] = 0.4   \n",
    "    \n",
    "    patch_sz = 224 // G\n",
    "    feats = []\n",
    "    for box in boxes.cpu():\n",
    "        x0,y0,x1,y1 = box.tolist()\n",
    "        sx, sy = 224/image_pil.width, 224/image_pil.height\n",
    "        x0*=sx; x1*=sx; y0*=sy; y1*=sy\n",
    "        cx0, cy0 = int(x0//patch_sz), int(y0//patch_sz)\n",
    "        cx1, cy1 = int((x1-1)//patch_sz), int((y1-1)//patch_sz)\n",
    "        ids = [r*G+c\n",
    "               for r in range(cy0, cy1+1)\n",
    "               for c in range(cx0, cx1+1)\n",
    "               if 0<=r<G and 0<=c<G]\n",
    "        if not ids:\n",
    "            feats.append(torch.zeros(patch_tok.size(-1)))\n",
    "            continue\n",
    "        w = mid_simi[ids][:, ids].sum(1)\n",
    "        f = (w[:, None] * patch_tok[ids]).sum(0) / w.sum()\n",
    "        feats.append(F.normalize(f, dim=0))        \n",
    "    \n",
    "    return torch.stack(feats).to(device)\n",
    "\n",
    "def clip_text_feat(text, processor, model, device):\n",
    "    \"\"\"Extract CLIP text features and normalize.\"\"\"\n",
    "    enc = processor(text=[text], return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        txt = model.get_text_features(**enc)\n",
    "    return F.normalize(txt.squeeze(0), dim=-1)\n",
    "\n",
    "def scclip_box_feat(pil_img, box, scclip, clip_model, device):\n",
    "    \"\"\"Extract SC-CLIP box features and project to CLIP embedding space.\"\"\"\n",
    "    tok768 = extract_scclip_box_features(\n",
    "                scclip, pil_img, box.unsqueeze(0), device=device\n",
    "             ).squeeze(0)\n",
    "\n",
    "    tok768 = tok768.clone()\n",
    "\n",
    "    if hasattr(clip_model, \"visual\"):\n",
    "        img512 = tok768 @ clip_model.visual.proj\n",
    "    else:\n",
    "        img512 = clip_model.visual_projection(tok768)\n",
    "\n",
    "    return F.normalize(img512, dim=-1)\n",
    "\n",
    "def scclip_box_feat_to_class(pil_img, box, scclip, clip_model, device):\n",
    "    \"\"\"Extract SC-CLIP box features with LayerNorm and project to CLIP embedding space.\"\"\"\n",
    "    tok768 = extract_scclip_box_features(\n",
    "                scclip, pil_img, box.unsqueeze(0), device=device\n",
    "             ).squeeze(0)\n",
    "\n",
    "    tok768 = tok768.clone()\n",
    "\n",
    "    if hasattr(clip_model, \"visual\"):\n",
    "        tok768 = clip_model.visual.ln_post(tok768)\n",
    "        img512 = tok768 @ clip_model.visual.proj\n",
    "    else:\n",
    "        tok768 = clip_model.vision_model.post_layernorm(tok768)\n",
    "        img512 = clip_model.visual_projection(tok768)\n",
    "\n",
    "    return F.normalize(img512, dim=-1)\n",
    "\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def extract_scclip_mask_features_per_object(\n",
    "        model: torch.nn.Module,\n",
    "        image_pil: Image.Image,\n",
    "        masks: torch.Tensor,\n",
    "        *,\n",
    "        device=\"cpu\",\n",
    "        mid_idx=8,\n",
    "        beta=0.4,\n",
    "        top_k=10\n",
    "    ) -> torch.Tensor:\n",
    "    \"\"\"Extract SC-CLIP embeddings for each connected component in binary masks.\"\"\"\n",
    "    if masks.ndim == 2:\n",
    "        masks = masks.unsqueeze(0)\n",
    "    K, H, W = masks.shape\n",
    "    masks = masks.bool()\n",
    "\n",
    "    inp = resize(image_pil, (224,224), interpolation=Image.BICUBIC)\n",
    "    px  = pil_to_tensor(inp).float().unsqueeze(0).to(device) / 255.\n",
    "\n",
    "    penult = _get_tokens(model, px, layer_idx=-2)\n",
    "    last   = _get_tokens(model, px, layer_idx=-1)\n",
    "    mid    = _get_tokens(model, px, layer_idx=mid_idx)\n",
    "\n",
    "    G = int(math.sqrt(penult.size(1)-1))\n",
    "    penult = _resolve_anomaly(penult, G, top_k)\n",
    "    patch_tok = _self_adjust(\n",
    "        last[:,1:,:], mid[:,1:,:], beta\n",
    "    ).squeeze(0)\n",
    "\n",
    "    m224 = F.interpolate(\n",
    "        masks.float().unsqueeze(1),\n",
    "        size=(224,224), mode='nearest'\n",
    "    ).squeeze(1)\n",
    "    patch_masks = F.interpolate(\n",
    "        m224.unsqueeze(1), size=(G,G),\n",
    "        mode='nearest'\n",
    "    ).squeeze(1).bool()\n",
    "\n",
    "    feats = []\n",
    "    for k in range(K):\n",
    "        lab, num = cc_label(patch_masks[k].cpu().numpy().astype(np.uint8))\n",
    "        for lbl in range(1, num+1):\n",
    "            comp = (lab == lbl)\n",
    "            comp_ids = torch.tensor(comp, device=device).nonzero(as_tuple=False)\n",
    "            ids = comp_ids[:,0] * G + comp_ids[:,1]\n",
    "            if ids.numel()>0:\n",
    "                f = patch_tok[ids].mean(0)\n",
    "                feats.append(F.normalize(f,dim=0))\n",
    "    if len(feats)==0:\n",
    "        return torch.zeros((0, patch_tok.size(-1)), device=device)\n",
    "    return torch.stack(feats)\n",
    "\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def build_support_box_feats(\n",
    "        clip_model,\n",
    "        support_imgs,\n",
    "        support_boxes,\n",
    "        *,\n",
    "        device=\"cpu\"):\n",
    "    \"\"\"Build support box features by concatenating all box features from all shots.\"\"\"\n",
    "    all_feats = []\n",
    "    shot = support_imgs.size(0)\n",
    "\n",
    "    for s in range(shot):\n",
    "        img_tensor  = support_imgs[s]\n",
    "        boxes_tensor = support_boxes[s]\n",
    "\n",
    "        img_pil = Image.fromarray(\n",
    "            (img_tensor.cpu().permute(1, 2, 0).numpy() * 255).astype(\"uint8\")\n",
    "        )\n",
    "\n",
    "        if boxes_tensor.ndim == 1:\n",
    "            boxes_tensor = boxes_tensor.unsqueeze(0)\n",
    "\n",
    "        if boxes_tensor.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        feats = extract_scclip_box_features(\n",
    "                    clip_model,\n",
    "                    img_pil,\n",
    "                    boxes_tensor.to(device),\n",
    "                    device=device)\n",
    "        all_feats.append(feats)\n",
    "\n",
    "    if len(all_feats) == 0:\n",
    "        D = clip_model.config.projection_dim\n",
    "        return torch.zeros((0, D), device=device)\n",
    "\n",
    "    return torch.cat(all_feats, dim=0)\n",
    "\n",
    "\n",
    "def visualize_filtered_boxes(image_pil, boxes, title=\"Filtered Boxes\"):\n",
    "    \"\"\"Visualize filtered boxes on image.\"\"\"\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "    ax.imshow(image_pil)\n",
    "\n",
    "    if isinstance(boxes, torch.Tensor):\n",
    "        boxes = boxes.tolist()\n",
    "\n",
    "    for box in boxes:\n",
    "        x_min, y_min, x_max, y_max = map(int, box)\n",
    "        width, height = x_max - x_min, y_max - y_min\n",
    "        rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x_min, y_min - 5, f\"Box: [{x_min}, {y_min}, {x_max}, {y_max}]\", color='white',\n",
    "                bbox=dict(facecolor='red', alpha=0.5), fontsize=8)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def clip_mask_emb(pil_img: Image.Image,\n",
    "                  masks,\n",
    "                  clip_processor, clip_model,\n",
    "                  device=\"cpu\"):\n",
    "    \"\"\"Extract CLIP embeddings for each mask by cropping and masking image regions.\"\"\"\n",
    "    if isinstance(masks, Image.Image):\n",
    "        masks = pil_to_tensor(masks)\n",
    "\n",
    "    m = masks.clone()\n",
    "    if m.dim() == 2:\n",
    "        m = m.unsqueeze(0)\n",
    "    m = (m.float() > 0).to(torch.bool)\n",
    "\n",
    "    K, H, W = m.shape\n",
    "    feats   = []\n",
    "\n",
    "    np_img = np.array(pil_img)\n",
    "\n",
    "    for k in range(K):\n",
    "        mk = m[k]\n",
    "\n",
    "        if mk.sum() == 0:\n",
    "            feats.append(torch.zeros(512, device=device))\n",
    "            continue\n",
    "\n",
    "        ys, xs  = torch.where(mk)\n",
    "        y0, y1  = ys.min().item(), ys.max().item() + 1\n",
    "        x0, x1  = xs.min().item(), xs.max().item() + 1\n",
    "\n",
    "        crop_arr          = np_img[y0:y1, x0:x1].copy()\n",
    "        mk_crop           = mk[y0:y1, x0:x1].cpu().numpy()\n",
    "        crop_arr[~mk_crop] = 0\n",
    "\n",
    "        crop_pil = Image.fromarray(crop_arr)\n",
    "\n",
    "        enc = clip_processor(images=crop_pil,\n",
    "                             return_tensors=\"pt\").to(device)\n",
    "        img_feat = clip_model.get_image_features(**enc)\n",
    "        feats.append(F.normalize(img_feat.squeeze(0), dim=-1))\n",
    "\n",
    "    return torch.stack(feats)\n",
    "\n",
    "\n",
    "def extract_local_features_with_mask(model, processor, image_pil, mask, device):\n",
    "    \"\"\"Extract local features by masking image and extracting CLIP features.\"\"\"\n",
    "    mask_resized = torch.nn.functional.interpolate(mask.unsqueeze(0).unsqueeze(0).float(), \n",
    "                                                  size=image_pil.size[::-1], \n",
    "                                                  mode='nearest').squeeze().long()\n",
    "    mask_np = mask_resized.cpu().numpy()\n",
    "    masked_image = np.array(image_pil).copy()\n",
    "    masked_image[mask_np == 0] = 0\n",
    "    \n",
    "    masked_image_pil = Image.fromarray(masked_image)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        inputs = processor(images=masked_image_pil, return_tensors=\"pt\").to(device)\n",
    "        outputs = model.get_image_features(pixel_values=inputs[\"pixel_values\"])\n",
    "        local_features = outputs.squeeze(0)\n",
    "    \n",
    "    return local_features\n",
    "\n",
    "\n",
    "\n",
    "def visualize_pred_mask(support_img, support_mask, alpha=0.5, title='title'):\n",
    "    \"\"\"Visualize support mask overlaid on support image with red color.\"\"\"\n",
    "    if isinstance(support_img, torch.Tensor):\n",
    "        support_img_pil = Image.fromarray(\n",
    "            (support_img.squeeze(0).cpu().permute(1, 2, 0).numpy() * 255).astype('uint8')\n",
    "        )\n",
    "    else:\n",
    "        support_img_pil = support_img\n",
    "\n",
    "    if isinstance(support_mask, torch.Tensor):\n",
    "        mask_np = support_mask.cpu().numpy()\n",
    "    elif isinstance(support_mask, np.ndarray):\n",
    "        mask_np = support_mask\n",
    "    else:\n",
    "        raise TypeError(\"support_mask must be a torch.Tensor or numpy.ndarray\")\n",
    "\n",
    "    assert support_img_pil.size == (mask_np.shape[1], mask_np.shape[0]), \"Image and mask size mismatch\"\n",
    "\n",
    "    img_np = np.array(support_img_pil)\n",
    "\n",
    "    red_mask = np.zeros_like(img_np)\n",
    "    red_mask[..., 0] = 255\n",
    "\n",
    "    overlay = np.where(mask_np[..., None] > 0, red_mask, img_np)\n",
    "\n",
    "    blended = (img_np * (1 - alpha) + overlay * alpha).astype(np.uint8)\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(blended)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def extract_global_features_from_text(class_name, processor, model, device):\n",
    "    \"\"\"Extract global features from class name text using CLIP.\"\"\"\n",
    "    text = f\"a photo of a {class_name}\"\n",
    "    inputs = processor(text=text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.get_text_features(input_ids=inputs[\"input_ids\"])\n",
    "        global_features = outputs.squeeze(0)\n",
    "    \n",
    "    return global_features\n",
    "\n",
    "def extract_global_features_from_background(class_name, processor, model, device):\n",
    "    \"\"\"Extract global features from background text using CLIP.\"\"\"\n",
    "    text = f\"a photo of a background\"    \n",
    "    inputs = processor(text=text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.get_text_features(input_ids=inputs[\"input_ids\"])\n",
    "        global_features = outputs.squeeze(0)\n",
    "    \n",
    "    return global_features\n",
    "\n",
    "def extract_clip_box_features(clip_model, query_img_tensor, boxes, device):\n",
    "    \"\"\"Extract CLIP box features by cropping image regions and extracting [CLS] token features.\"\"\"\n",
    "    clip_model.eval()\n",
    "    query_img_tensor = query_img_tensor.to(device)\n",
    "    box_features = []\n",
    "\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = box.int()\n",
    "        cropped = crop(query_img_tensor, y1.item(), x1.item(), (y2 - y1).item(), (x2 - x1).item())\n",
    "        cropped_pil = transforms.ToPILImage()(cropped.cpu())\n",
    "\n",
    "        inputs = clip_processor(images=cropped_pil, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            vision_outputs = clip_model.vision_model(**inputs.vision_model_input)\n",
    "            feature = vision_outputs.last_hidden_state[:, 0, :]\n",
    "            feature = F.normalize(feature, dim=-1)\n",
    "\n",
    "        box_features.append(feature.squeeze(0))\n",
    "\n",
    "    if box_features:\n",
    "        return torch.stack(box_features, dim=0)\n",
    "    else:\n",
    "        return torch.zeros((0, clip_model.config.projection_dim), device=device)\n",
    "\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    utils_seed.fix_randseed(0)\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Define transformations and dataset\n",
    "    transform = Compose([\n",
    "        ToTensor()])\n",
    "\n",
    "    coco_class_names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
    "\n",
    "    datapath = \"./data\"\n",
    "    fold = 1\n",
    "    split = \"val\"\n",
    "    shot = 1\n",
    "    use_original_imgsize = False\n",
    "\n",
    "    dataset = DatasetCOCO(datapath, fold, transform, split, shot, use_original_imgsize)\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    processor = Owlv2Processor.from_pretrained(\"google/owlv2-large-patch14-ensemble\")\n",
    "    model = Owlv2ForObjectDetection.from_pretrained(\"google/owlv2-large-patch14-ensemble\")\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "    clip_model     = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\").to(device)\n",
    "    clip_model.config.output_hidden_states = True\n",
    "    clip_model.vision_model.return_dict     = True\n",
    "\n",
    "    sam_checkpoint = \"./sam_checkpoints/sam_vit_h_4b8939.pth\"\n",
    "    sam_model_type = \"vit_h\"\n",
    "    sam = sam_model_registry[sam_model_type](checkpoint=sam_checkpoint)\n",
    "    sam.to(device)\n",
    "    predictor = SamPredictor(sam)\n",
    "\n",
    "    total_miou = 0.0\n",
    "    num_batches = 0\n",
    "    total_batches = len(dataloader)\n",
    "    Evaluator.initialize()\n",
    "    average_meter = AverageMeter(dataloader.dataset)\n",
    "    \n",
    "\n",
    "    for i, batch in enumerate(dataloader, start=1):\n",
    "        batch = utils_seed.to_cuda(batch)\n",
    "        query_img = batch['query_img'].squeeze(0).to(device)\n",
    "        query_mask = batch['query_mask'].squeeze(0).to(device)\n",
    "        query_img_pil = Image.fromarray((query_img.cpu().permute(1, 2, 0).numpy() * 255).astype('uint8'))\n",
    "        \n",
    "        support_imgs   = batch['support_imgs'][0].to(device)\n",
    "        support_boxes  = batch['support_boxes'][0]\n",
    "        support_masks  = batch['support_masks'][0].to(device)\n",
    "\n",
    "        support_embs = []\n",
    "        for s in range(support_imgs.size(0)):\n",
    "            sup_pil = Image.fromarray(\n",
    "                (support_imgs[s].cpu().permute(1,2,0).numpy() * 255).astype(\"uint8\")\n",
    "            )\n",
    "            mask_np = support_masks[s].cpu().numpy().astype(bool)\n",
    "            img_np  = np.array(sup_pil)\n",
    "            img_np[~mask_np] = 0\n",
    "            sup_crop = Image.fromarray(img_np)\n",
    "\n",
    "            enc_sup = clip_processor(images=sup_crop, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                emb = clip_model.get_image_features(**enc_sup)\n",
    "            support_embs.append(F.normalize(emb.squeeze(0), dim=-1))\n",
    "\n",
    "        support_emb = torch.stack(support_embs, dim=0).mean(0)\n",
    "\n",
    "        class_id = batch['class_id']\n",
    "        class_name = coco_class_names[class_id]\n",
    "        lines = [[f'a photo of a {class_name}']]\n",
    "\n",
    "        text_inputs = clip_processor(\n",
    "            text=[f\"a photo of a {class_name}\"], return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "        with torch.no_grad():\n",
    "            class_emb = clip_model.get_text_features(**text_inputs)\n",
    "        class_emb = F.normalize(class_emb.squeeze(0), dim=-1)\n",
    "\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = processor(text=lines, images=query_img_pil, return_tensors=\"pt\").to(device)\n",
    "            outputs = model(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                pixel_values=inputs[\"pixel_values\"],\n",
    "                attention_mask=inputs[\"attention_mask\"]\n",
    "            )\n",
    "\n",
    "        target_sizes = torch.tensor([query_img_pil.size[::-1]], dtype=torch.float32).to(device)\n",
    "        results = processor.post_process_object_detection(\n",
    "            outputs=outputs,\n",
    "            target_sizes=target_sizes,\n",
    "            threshold=0.08\n",
    "        )\n",
    "\n",
    "        pred_boxes = results[0][\"boxes\"].to(device)\n",
    "        pred_scores = results[0][\"scores\"].to(device)            \n",
    "\n",
    "        if pred_boxes.shape[0] > 0:\n",
    "            s_min, s_max = pred_scores.min().item(), pred_scores.max().item()\n",
    "            denom = max(s_max - s_min, 1e-6)\n",
    "\n",
    "            reweighted_scores = []\n",
    "            for box, orig_score in zip(pred_boxes, pred_scores):\n",
    "                x0, y0, x1, y1 = map(int, box.tolist())\n",
    "                crop = query_img_pil.crop((x0, y0, x1, y1))\n",
    "\n",
    "                cropped_np = np.array(crop)\n",
    "                if cropped_np.ndim == 3:\n",
    "                    h, w, c = cropped_np.shape\n",
    "                    if h == 1 and c == 3:\n",
    "                        cropped_np = cropped_np[0]\n",
    "                    elif c == 1:\n",
    "                        cropped_np = np.concatenate([cropped_np]*3, axis=2)\n",
    "                elif cropped_np.ndim == 2:\n",
    "                    cropped_np = np.stack([cropped_np]*3, axis=-1)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported cropped image shape: {cropped_np.shape}\")\n",
    "                crop_rgb = Image.fromarray(cropped_np.astype(np.uint8)).convert(\"RGB\")\n",
    "\n",
    "                enc = clip_processor(images=crop_rgb, return_tensors=\"pt\")\n",
    "                enc = {k: v.to(device) for k, v in enc.items()}\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    img_emb = clip_model.get_image_features(**enc)\n",
    "                img_emb = F.normalize(img_emb.squeeze(0), dim=-1)\n",
    "\n",
    "                sim_text    = (img_emb @ class_emb).item()\n",
    "                sim_support = (img_emb @ support_emb).item()\n",
    "                score_norm  = (orig_score.item() - s_min) / denom\n",
    "                \n",
    "                final_score = (sim_text + sim_support + score_norm) / 3\n",
    "                reweighted_scores.append(final_score)\n",
    "\n",
    "            reweighted = torch.tensor(reweighted_scores, device=pred_boxes.device)\n",
    "            tau = reweighted.mean() + 0.3 * reweighted.std()\n",
    "            keep = reweighted > tau\n",
    "            filtering_box = pred_boxes[keep]\n",
    "    \n",
    "    \n",
    "        if len(filtering_box) > 0:\n",
    "            filtering_box = filtering_box\n",
    "        else:\n",
    "            filtering_box = pred_boxes\n",
    "            print('not_filterd_box')\n",
    "\n",
    "            \n",
    "        predictor.set_image(np.array(query_img_pil))\n",
    "        \n",
    "        masks_list = []\n",
    "\n",
    "        if isinstance(filtering_box, torch.Tensor) and filtering_box.numel() > 0:\n",
    "            for box in filtering_box.cpu().numpy():\n",
    "                x_min, y_min, x_max, y_max = map(int, box)\n",
    "                cx = (x_min + x_max) // 2\n",
    "                cy = (y_min + y_max) // 2\n",
    "\n",
    "                masks, scores, low_res_logits = predictor.predict(\n",
    "                    box=box,\n",
    "                    multimask_output=True\n",
    "                )\n",
    "                best0 = np.argmax(scores)  \n",
    "                init_mask      = masks[best0]\n",
    "                init_logits    = low_res_logits[best0:best0+1]\n",
    "\n",
    "                masks1, scores1, logits1 = predictor.predict(\n",
    "                    box=box,\n",
    "                    mask_input=init_logits,\n",
    "                    multimask_output=True\n",
    "                )\n",
    "                \n",
    "    \n",
    "                best1 = np.argmax(scores1)\n",
    "                mask1      = masks1[best1]\n",
    "                logits1_in = logits1[best1:best1+1]\n",
    "                     \n",
    "                                \n",
    "                ys, xs = np.nonzero(mask1)\n",
    "                if xs.size == 0 or ys.size == 0:\n",
    "                    masks_list.append(mask1)\n",
    "                    break\n",
    "                \n",
    "                x0, x1 = xs.min(), xs.max()\n",
    "                y0, y1 = ys.min(), ys.max()\n",
    "                refine_box = np.array([x0, y0, x1, y1])[None, :]\n",
    "\n",
    "                masks2, scores2, logits2 = predictor.predict(\n",
    "                    box=refine_box,\n",
    "                    mask_input=logits1_in,\n",
    "                    multimask_output=True\n",
    "                )\n",
    "                \n",
    "                mask_tensor = torch.from_numpy(masks2).to(device)\n",
    "\n",
    "                all_mask_feats = []\n",
    "                for k in range(mask_tensor.shape[0]):\n",
    "                    single_mask = mask_tensor[k]\n",
    "                    feat_k = extract_local_features_with_mask(\n",
    "                        clip_model,\n",
    "                        clip_processor,\n",
    "                        query_img_pil,\n",
    "                        single_mask,\n",
    "                        device=device\n",
    "                    )\n",
    "                    all_mask_feats.append(feat_k)\n",
    "\n",
    "                mask_feats = torch.stack(all_mask_feats, dim=0)\n",
    "                scores2_t = torch.from_numpy(scores2).to(mask_feats.device)\n",
    "                \n",
    "                \n",
    "                mask_feats = F.normalize(mask_feats, dim=-1)\n",
    "                class_emb  = F.normalize(class_emb, dim=-1)\n",
    "\n",
    "                mask_sims = torch.matmul(mask_feats, class_emb)\n",
    "\n",
    "                \n",
    "                best2      = np.argmax(scores2)\n",
    "                best_idx  = mask_sims.argmax().item()\n",
    "                final_mask = masks2[best2]\n",
    "       \n",
    "                \n",
    "                \n",
    "                best_sim = mask_sims[best_idx]\n",
    "                \n",
    "                if best_sim < 0.2:\n",
    "                    continue\n",
    "                                \n",
    "                \n",
    "\n",
    "                if final_mask.sum() == 0:\n",
    "                    masks_list.append(mask1)\n",
    "                else:\n",
    "                    masks_list.append(final_mask)\n",
    "    \n",
    "                \n",
    "\n",
    "                    \n",
    "        \n",
    "        else:\n",
    "            height, width = query_img_pil.size[::-1]\n",
    "            full_image_box = np.array([[0, 0, width, height]])\n",
    "            mask, _, _ = predictor.predict(box=full_image_box)\n",
    "            masks_list.append(mask[0])\n",
    "\n",
    "            \n",
    "        if isinstance(filtering_box, torch.Tensor) and filtering_box.numel() > 0 and len(masks_list) == 0:\n",
    "            for box in filtering_box.cpu().numpy():\n",
    "\n",
    "                x_min, y_min, x_max, y_max = map(int, box)\n",
    "                cx = (x_min + x_max) // 2\n",
    "                cy = (y_min + y_max) // 2\n",
    "                topk_xy = np.array([[cx, cy]])\n",
    "                topk_label = np.array([1])\n",
    "\n",
    "                # [Step 1] First-step prediction: 단일 마스크 예측 (multimask_output=True)\n",
    "                masks, scores, logits = predictor.predict(\n",
    "                    box=box,\n",
    "                    multimask_output=True\n",
    "                )            \n",
    "                masks_list.extend(masks)   \n",
    "            \n",
    "        \n",
    "\n",
    "        combined_mask = np.any(np.stack(masks_list), axis=0).astype(np.float32)\n",
    "        combined_mask_tensor = torch.tensor(combined_mask, dtype=torch.float32).to(device)\n",
    "        \n",
    "        \n",
    "        if query_mask.shape != combined_mask_tensor.shape:\n",
    "            query_mask_resized = torch.nn.functional.interpolate(\n",
    "                query_mask.unsqueeze(0).unsqueeze(0).float(),\n",
    "                size=combined_mask_tensor.shape,\n",
    "                mode=\"nearest\"\n",
    "            ).squeeze().to(device)            \n",
    "        else:\n",
    "            query_mask_resized = query_mask\n",
    "    \n",
    "\n",
    "        pred_mask_batch = combined_mask_tensor.unsqueeze(0)\n",
    "        query_mask_batch = query_mask_resized.unsqueeze(0)\n",
    "        batch['query_mask'] = query_mask_batch\n",
    "        \n",
    "        pred_mask_batch = pred_mask_batch.to(device)\n",
    "        query_mask_batch = query_mask_batch.to(device)\n",
    "        \n",
    "        area_inter, area_union = Evaluator.classify_prediction(pred_mask_batch, batch)\n",
    "        average_meter.update(area_inter, area_union, batch['class_id'], loss=None)\n",
    "        average_meter.write_process(i, total_batches, epoch=-1, write_batch_idx=1)\n",
    "\n",
    "    average_meter.write_result('Test', 0)\n",
    "    miou, fb_iou, _ = average_meter.compute_iou()\n",
    "\n",
    "    logger.info(f\"Fold {fold} mIoU: {miou:.2f} \\t FB-IoU: {fb_iou:.2f}\")\n",
    "    logger.info('==================== Finished Testing ====================')\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
